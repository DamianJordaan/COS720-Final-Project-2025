{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "488d9753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\djjor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "import joblib\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import tldextract\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5110b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # install tldextract if not already installed\n",
    "# try:\n",
    "#     import tldextract\n",
    "# except ImportError:\n",
    "#     import pip\n",
    "#     pip.main(['install', 'tldextract'])\n",
    "#     import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e95f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhishingEmailDetector:\n",
    "    def __init__(self, classifier: BaseEstimator = None):\n",
    "        self.text_col = 'combined_text'\n",
    "        self.meta_cols = ['same_domain', 'num_links', 'num_suspicious_words']\n",
    "        self.vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "        self.scaler = StandardScaler()\n",
    "        # self.classifier = classifier or LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "        self.classifier = classifier\n",
    "        self.pipeline = None\n",
    "\n",
    "        if self.classifier is None:\n",
    "            self.classifier = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "\n",
    "    # ---- Text Cleaning ----\n",
    "    def _clean_text(self, text):\n",
    "        if pd.isnull(text):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"<.*?>\", \" \", text)\n",
    "        text = re.sub(r\"http\\S+\", \"LINKURL\", text)\n",
    "        # text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text) # convert to lowercase\n",
    "        text = re.sub(r\"\\d+\", \"NUM\", text) # remove numbers\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip() # remove extra spaces\n",
    "        return \" \".join([w for w in text.split() if w not in stop_words])\n",
    "\n",
    "    # ---- Metadata ----\n",
    "    def _extract_domain(self, email):\n",
    "        if pd.isnull(email) or \"@\" not in email:\n",
    "            return \"unknown\"\n",
    "        return email.split('@')[-1].lower()\n",
    "\n",
    "    def _is_same_domain(self, sender, recipient):\n",
    "        return int(self._extract_domain(sender) == self._extract_domain(recipient))\n",
    "\n",
    "    def _count_links(self, text):\n",
    "        if pd.isnull(text):\n",
    "            return 0\n",
    "        return len(re.findall(r\"http[s]?://\", text))\n",
    "\n",
    "    def _count_suspicious_words(self, text, keywords=None):\n",
    "        if pd.isnull(text):\n",
    "            return 0\n",
    "        if keywords is None:\n",
    "            keywords = [\"verify\", \"login\", \"click\", \"update\", \"urgent\", \"password\"]\n",
    "        return sum(word in text.lower() for word in keywords)\n",
    "\n",
    "    # ---- Preprocessing ----\n",
    "    def preprocess_dataframe(self, dfx, debug=False):\n",
    "        df = dfx.copy()\n",
    "        df['clean_subject'] = df['subject'].apply(self._clean_text)\n",
    "        df['clean_body'] = df['body'].apply(self._clean_text)\n",
    "        df[self.text_col] = df['clean_subject'] + \" \" + df['clean_body']\n",
    "        df['same_domain'] = df.apply(lambda x: self._is_same_domain(x['sender'], x['recipient']), axis=1)\n",
    "        df['num_links'] = df['body'].apply(self._count_links)\n",
    "        df['num_suspicious_words'] = df['body'].apply(self._count_suspicious_words)\n",
    "\n",
    "        if debug:\n",
    "            print(\"Cleaned Text:\")\n",
    "            print(df[self.text_col].head())\n",
    "            print(\"Metadata:\")\n",
    "            print(df[self.meta_cols].head())\n",
    "\n",
    "        return df[[self.text_col] + self.meta_cols]\n",
    "\n",
    "    # ---- Build Pipeline ----\n",
    "    def _build_pipeline(self):\n",
    "        transformer = ColumnTransformer([\n",
    "            ('tfidf', self.vectorizer, self.text_col),\n",
    "            ('meta', self.scaler, self.meta_cols)\n",
    "        ])\n",
    "        return Pipeline([\n",
    "            ('features', transformer),\n",
    "            ('clf', self.classifier)\n",
    "        ])\n",
    "\n",
    "    # ---- Train Model ----\n",
    "    def fit(self, df, labels, debug=False):\n",
    "        processed = self.preprocess_dataframe(df, debug=debug)\n",
    "\n",
    "        self.pipeline = self._build_pipeline()\n",
    "        self.pipeline.fit(processed, labels)\n",
    "\n",
    "    # ---- Predict Labels ----\n",
    "    def predict(self, df, debug=False):\n",
    "        processed = self.preprocess_dataframe(df, debug=debug)\n",
    "        return self.pipeline.predict(processed)\n",
    "\n",
    "    def predict_proba(self, df):\n",
    "        processed = self.preprocess_dataframe(df)\n",
    "        return self.pipeline.predict_proba(processed)\n",
    "\n",
    "    # ---- Evaluate ----\n",
    "    def evaluate(self, df, labels):\n",
    "        preds = self.predict(df)\n",
    "        print(classification_report(labels, preds))\n",
    "\n",
    "    # ---- Get pipeline for advanced use ----\n",
    "    def get_pipeline(self):\n",
    "        return self.pipeline\n",
    "\n",
    "    def explain_instance(self, df_row, num_features=10):\n",
    "        \"\"\"\n",
    "        Explains a single email instance (df_row must be a one-row DataFrame).\n",
    "        Returns explanation object with weights.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"You must fit the model before calling explain_instance.\")\n",
    "\n",
    "        # Prepare LimeTextExplainer\n",
    "        class_names = ['Legitimate', 'Phishing']\n",
    "        explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "        # Extract text from row\n",
    "        email_text = self._clean_text(df_row['subject'].values[0]) + \" \" + self._clean_text(df_row['body'].values[0])\n",
    "\n",
    "        # Build a prediction function that LIME can use\n",
    "        def predict_proba(texts):\n",
    "            # texts: list of raw texts from LIME\n",
    "            temp_df = pd.DataFrame({\n",
    "                'subject': [''] * len(texts),\n",
    "                'body': texts,\n",
    "                'sender': [df_row['sender'].values[0]] * len(texts),\n",
    "                'recipient': [df_row['recipient'].values[0]] * len(texts)\n",
    "            })\n",
    "            return self.predict_proba(temp_df)\n",
    "\n",
    "        # Explain\n",
    "        exp = explainer.explain_instance(email_text, predict_proba, num_features=num_features)\n",
    "        return exp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aaf5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the datasets\n",
    "\n",
    "dir_path1 = \"archive/CEAS_08.csv\" # sender,receiver,date,subject,body,label,urls\n",
    "dir_path2 = \"archive/Enron.csv\" # subject,body,label\n",
    "dir_path3 = \"archive/Ling.csv\" # subject,body,label\n",
    "dir_path4 = \"archive/Nazario.csv\" # sender,receiver,date,subject,body,urls,label\n",
    "dir_path5 = \"archive/Nigerian_Fraud.csv\" # sender,receiver,date,subject,body,urls,label\n",
    "dir_path6 = \"archive/SpamAssasin.csv\" # sender,receiver,date,subject,body,label,urls\n",
    "\n",
    "df1 = pd.read_csv(dir_path1, encoding='latin-1')\n",
    "df2 = pd.read_csv(dir_path2, encoding='latin-1')\n",
    "df3 = pd.read_csv(dir_path3, encoding='latin-1')\n",
    "df4 = pd.read_csv(dir_path4, encoding='latin-1')\n",
    "df5 = pd.read_csv(dir_path5, encoding='latin-1')\n",
    "df6 = pd.read_csv(dir_path6, encoding='latin-1')\n",
    "\n",
    "# fill missing values for df2, df3 (sender = sender@sender.s, receiver = receiver@receiver.r)\n",
    "# add sender and receiver columns to df2, df3\n",
    "df2['sender'] = df2['subject'].apply(lambda x: 'unknown@sender.s' if pd.isnull(x) else 'unknown@sender.s')\n",
    "df2['receiver'] = df2['subject'].apply(lambda x: 'unknown@receiver.r' if pd.isnull(x) else 'unknown@receiver.r')\n",
    "\n",
    "# df2['sender'] = df2['sender'].fillna('unknown@sender.s')\n",
    "# df2['receiver'] = df2['receiver'].fillna('unknown@receiver.r')\n",
    "\n",
    "df3['sender'] = df3['subject'].apply(lambda x: 'unknown@sender.s' if pd.isnull(x) else 'unknown@sender.s')\n",
    "df3['receiver'] = df3['subject'].apply(lambda x: 'unknown@receiver.r' if pd.isnull(x) else 'unknown@receiver.r')\n",
    "\n",
    "# df3['sender'] = df3['sender'].fillna('unknown@sender.s')\n",
    "# df3['receiver'] = df3['receiver'].fillna('unknown@receiver.r')\n",
    "\n",
    "# merge the datasets\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)\n",
    "df = df.dropna(subset=['body', 'subject', 'sender', 'receiver', 'label'])\n",
    "# rename columns for consistency\n",
    "df = df.rename(columns={'sender': 'sender', 'receiver': 'recipient', 'subject': 'subject', 'body': 'body', 'label': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867b0439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 79789 entries, 0 to 82485\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   sender     79789 non-null  object \n",
      " 1   recipient  79789 non-null  object \n",
      " 2   date       47290 non-null  object \n",
      " 3   subject    79789 non-null  object \n",
      " 4   body       79789 non-null  object \n",
      " 5   label      79789 non-null  int64  \n",
      " 6   urls       47423 non-null  float64\n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 4.9+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                              sender  \\\n",
       " 0                   Young Esposito <Young@iworld.de>   \n",
       " 1                       Mok <ipline's1983@icable.ph>   \n",
       " 2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
       " 3                 Michael Parker <ivqrnai@pobox.com>   \n",
       " 4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
       " \n",
       "                                         recipient  \\\n",
       " 0                     user4@gvc.ceas-challenge.cc   \n",
       " 1                   user2.2@gvc.ceas-challenge.cc   \n",
       " 2                   user2.9@gvc.ceas-challenge.cc   \n",
       " 3  SpamAssassin Dev <xrh@spamassassin.apache.org>   \n",
       " 4                   user2.2@gvc.ceas-challenge.cc   \n",
       " \n",
       "                               date  \\\n",
       " 0  Tue, 05 Aug 2008 16:31:02 -0700   \n",
       " 1  Tue, 05 Aug 2008 18:31:03 -0500   \n",
       " 2  Tue, 05 Aug 2008 20:28:00 -1200   \n",
       " 3  Tue, 05 Aug 2008 17:31:20 -0600   \n",
       " 4  Tue, 05 Aug 2008 19:31:21 -0400   \n",
       " \n",
       "                                              subject  \\\n",
       " 0                          Never agree to be a loser   \n",
       " 1                             Befriend Jenna Jameson   \n",
       " 2                               CNN.com Daily Top 10   \n",
       " 3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
       " 4                         SpecialPricesPharmMoreinfo   \n",
       " \n",
       "                                                 body  label  urls  \n",
       " 0  Buck up, your troubles caused by small dimensi...      1   1.0  \n",
       " 1  \\nUpgrade your sex and pleasures with these te...      1   1.0  \n",
       " 2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1   1.0  \n",
       " 3  Would anyone object to removing .so from this ...      0   1.0  \n",
       " 4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...      1   1.0  ,\n",
       " None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(), df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d82d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset to a csv file\n",
    "df.to_csv('phishing_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb29f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X = df.drop(columns='label')\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "510a99ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      9728\n",
      "           1       0.98      0.99      0.99     10220\n",
      "\n",
      "    accuracy                           0.99     19948\n",
      "   macro avg       0.99      0.99      0.99     19948\n",
      "weighted avg       0.99      0.99      0.99     19948\n",
      "\n",
      "Predictions: [1 0 0 ... 1 0 0]\n",
      "Precision: 0.98, Recall: 0.99, F1-Score: 0.99, Accuracy: 0.99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['phishing_email_detector_v2.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "# df = pd.DataFrame({\n",
    "#     'subject': [\"URGENT: Reset your password\", \"Meeting tomorrow\"],\n",
    "#     'body': [\"Click here to update your credentials http://fake.site\", \"Let's talk at 10am.\"],\n",
    "#     'sender': [\"support@fakebank.com\", \"teammate@realco.com\"],\n",
    "#     'recipient': [\"user@realco.com\", \"user@realco.com\"],\n",
    "#     'label': [1, 0]\n",
    "# })\n",
    "\n",
    "# Train detector\n",
    "from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# detector = PhishingEmailDetector(classifier=SVC(kernel='linear', class_weight='balanced', probability=True))\n",
    "detector = PhishingEmailDetector(classifier=SVC(kernel='rbf', class_weight='balanced', probability=True, C=10))\n",
    "\n",
    "# detector = PhishingEmailDetector()\n",
    "detector.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "detector.evaluate(X_test, y_test)\n",
    "\n",
    "# Predict on new sample\n",
    "pred = detector.predict(X_test)\n",
    "print(\"Predictions:\", pred)\n",
    "\n",
    "# calculate precision, recall, f1-score, accuracy of test data\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "precision = precision_score(y_test, pred)\n",
    "recall = recall_score(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Save the model using joblib\n",
    "joblib.dump(detector.get_pipeline(), 'phishing_email_detector_v2.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0a9d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = joblib.load('phishing_email_detector.pkl')\n",
    "detector = PhishingEmailDetector()\n",
    "detector.pipeline = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70be1b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text:\n",
      "0    urgent reset password click update credentials...\n",
      "1                      meeting tomorrow let talk NUMam\n",
      "Name: combined_text, dtype: object\n",
      "Metadata:\n",
      "   same_domain  num_links  num_suspicious_words\n",
      "0            0          1                     2\n",
      "1            1          0                     0\n",
      "Sample Predictions: [1 0]\n",
      "Single Email for Explanation:\n",
      "                       subject  \\\n",
      "0  URGENT: Reset your password   \n",
      "\n",
      "                                                body                sender  \\\n",
      "0  Click here to update your credentials http://f...  support@fakebank.com   \n",
      "\n",
      "         recipient  \n",
      "0  user@realco.com  \n",
      "LIME Explanation:\n",
      "[('click', 0.14174449848925708), ('urgent', 0.08058383371880797), ('password', -0.06707510812382113), ('LINKURL', 0.019657643624652745), ('update', -0.015312960764313533), ('credentials', 0.006474735282390534), ('reset', -0.0006840374240482625)]\n"
     ]
    }
   ],
   "source": [
    "# test classifier on example data\n",
    "# Sample data\n",
    "df_sample_data = pd.DataFrame({\n",
    "    'subject': [\"URGENT: Reset your password\", \"Meeting tomorrow\"],\n",
    "    'body': [\"Click here to update your credentials http://fake.site\", \"Let's talk at 10am.\"],\n",
    "    'sender': [\"support@fakebank.com\", \"teammate@realco.com\"],\n",
    "    'recipient': [\"user@realco.com\", \"user@realco.com\"],\n",
    "    'label': [1, 0]\n",
    "})\n",
    "\n",
    "X_test_sample = df_sample_data.drop(columns='label')\n",
    "y_test_sample = df_sample_data['label']\n",
    "\n",
    "pred_sample = detector.predict(X_test_sample, debug=True)\n",
    "print(\"Sample Predictions:\", pred_sample)\n",
    "\n",
    "single_email = X_test_sample.iloc[[0]]\n",
    "print(\"Single Email for Explanation:\")\n",
    "print(single_email)\n",
    "\n",
    "explination = detector.explain_instance(single_email, num_features=10)\n",
    "print(\"LIME Explanation:\")\n",
    "print(explination.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Define the classifiers and their hyperparameters\n",
    "classifiers = {\n",
    "    'SVC': SVC(class_weight='balanced'),\n",
    "    'RandomForest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'LogisticRegression': LogisticRegression(class_weight='balanced', solver='liblinear'),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'HistGradientBoosting': HistGradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Define the hyperparameters for each classifier\n",
    "\n",
    "param_grid = {\n",
    "    'SVC': {\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'clf__n_estimators': [50, 100],\n",
    "        'clf__max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__penalty': ['l2']\n",
    "    },\n",
    "    'MultinomialNB': {\n",
    "        # No hyperparameters to tune for MultinomialNB\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'clf__n_estimators': [50, 100],\n",
    "        'clf__learning_rate': [0.01, 0.1]\n",
    "    },\n",
    "    'HistGradientBoosting': {\n",
    "        'clf__max_iter': [50, 100],\n",
    "        'clf__learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "best_overall_model = None\n",
    "best_overall_score = 0\n",
    "best_overall_name = None\n",
    "\n",
    "def fitness_function(accuracy, precision, recall, f1):\n",
    "    return (accuracy + precision + recall + f1) / 4\n",
    "\n",
    "# Create a pipeline for each classifier using the PhishingEmailDetector class\n",
    "for name, clf in classifiers.items():\n",
    "    classifier = clf.__class__(class_weight='balanced', param_grid=param_grid[name])\n",
    "    detector = PhishingEmailDetector(classifier=classifier)\n",
    "    detector.fit(X_train, y_train)\n",
    "    detector.evaluate(X_test, y_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, detector.predict(X_test))\n",
    "    precision = precision_score(y_test, detector.predict(X_test))\n",
    "    recall = recall_score(y_test, detector.predict(X_test))\n",
    "    f1 = f1_score(y_test, detector.predict(X_test))\n",
    "\n",
    "    fitness = fitness_function(accuracy, precision, recall, f1)\n",
    "    print(f\"Classifier: {name}, Fitness: {fitness:.2f}\")\n",
    "    if fitness > best_overall_score:\n",
    "        best_overall_score = fitness\n",
    "        best_overall_model = detector.get_pipeline()\n",
    "        best_overall_name = name\n",
    "\n",
    "print(f\"Best overall model: {best_overall_name}, Score: {best_overall_score:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a3a326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: SVC, Params: {'C': 0.1, 'kernel': 'linear'}, Fitness: 0.97\n",
      "Classifier: SVC, Params: {'C': 0.1, 'kernel': 'rbf'}, Fitness: 0.97\n",
      "Classifier: SVC, Params: {'C': 1, 'kernel': 'linear'}, Fitness: 0.98\n",
      "Classifier: SVC, Params: {'C': 1, 'kernel': 'rbf'}, Fitness: 0.98\n",
      "Classifier: SVC, Params: {'C': 10, 'kernel': 'linear'}, Fitness: 0.98\n",
      "Classifier: SVC, Params: {'C': 10, 'kernel': 'rbf'}, Fitness: 0.99\n",
      "Classifier: RandomForest, Params: {'n_estimators': 50, 'max_depth': None}, Fitness: 0.98\n",
      "Classifier: RandomForest, Params: {'n_estimators': 50, 'max_depth': 10}, Fitness: 0.92\n",
      "Classifier: RandomForest, Params: {'n_estimators': 50, 'max_depth': 20}, Fitness: 0.95\n",
      "Classifier: RandomForest, Params: {'n_estimators': 100, 'max_depth': None}, Fitness: 0.98\n",
      "Classifier: RandomForest, Params: {'n_estimators': 100, 'max_depth': 10}, Fitness: 0.92\n",
      "Classifier: RandomForest, Params: {'n_estimators': 100, 'max_depth': 20}, Fitness: 0.95\n",
      "Classifier: LogisticRegression, Params: {'C': 0.1, 'penalty': 'l2'}, Fitness: 0.96\n",
      "Classifier: LogisticRegression, Params: {'C': 1, 'penalty': 'l2'}, Fitness: 0.98\n",
      "Classifier: LogisticRegression, Params: {'C': 10, 'penalty': 'l2'}, Fitness: 0.98\n",
      "Classifier: GradientBoosting, Params: {'n_estimators': 50, 'learning_rate': 0.01}, Fitness: 0.82\n",
      "Classifier: GradientBoosting, Params: {'n_estimators': 50, 'learning_rate': 0.1}, Fitness: 0.91\n",
      "Classifier: GradientBoosting, Params: {'n_estimators': 100, 'learning_rate': 0.01}, Fitness: 0.83\n",
      "Classifier: GradientBoosting, Params: {'n_estimators': 100, 'learning_rate': 0.1}, Fitness: 0.94\n",
      "Best overall model: SVC, Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # progress bar\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'SVC': SVC(class_weight='balanced', probability=True),\n",
    "    'RandomForest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'LogisticRegression': LogisticRegression(class_weight='balanced', solver='liblinear', ),\n",
    "    # 'MultinomialNB': MultinomialNB(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    # 'HistGradientBoosting': HistGradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grid = {\n",
    "    'SVC': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'RandomForest': {'n_estimators': [50, 100], 'max_depth': [None, 10, 20]},\n",
    "    'LogisticRegression': {'C': [0.1, 1, 10], 'penalty': ['l2']},\n",
    "    # 'MultinomialNB': {},\n",
    "    'GradientBoosting': {'n_estimators': [50, 100], 'learning_rate': [0.01, 0.1]}\n",
    "    # 'HistGradientBoosting': {'max_iter': [50, 100], 'learning_rate': [0.01, 0.1]}\n",
    "}\n",
    "\n",
    "# calc combinations of hyperparameters\n",
    "param_combinations = list(product(*param_grid.values()))\n",
    "\n",
    "# Initialize the progress bar\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas(desc=\"Processing\", unit=\"combination\", total=len(param_combinations))\n",
    "\n",
    "best_overall_model = None\n",
    "best_overall_score = 0\n",
    "best_overall_name = None\n",
    "\n",
    "def fitness_function(accuracy, precision, recall, f1):\n",
    "    \"\"\"\n",
    "    Custom fitness function prioritizing recall and precision.\n",
    "    - Recall is crucial for phishing detection (avoiding false negatives).\n",
    "    - Precision ensures fewer false positives (better user experience).\n",
    "    - F1-score balances them.\n",
    "    - Accuracy contributes but has the lowest weight.\n",
    "    \"\"\"\n",
    "    return (0.2 * accuracy) + (0.3 * precision) + (0.3 * recall) + (0.2 * f1)\n",
    "\n",
    "# Manual grid search\n",
    "for name, clf in classifiers.items():\n",
    "    # Generate hyperparameter combinations\n",
    "    param_combinations = list(product(*param_grid.get(name, {}).values()))\n",
    "    param_keys = list(param_grid.get(name, {}).keys())\n",
    "\n",
    "    for params in param_combinations:\n",
    "        param_dict = dict(zip(param_keys, params))\n",
    "\n",
    "        # Create classifier with specific hyperparameters\n",
    "        tuned_classifier = clf.__class__(**param_dict)\n",
    "\n",
    "        # print(hasattr(tuned_classifier, \"estimators_\"))\n",
    "\n",
    "        # Initialize phishing email detector\n",
    "        detector = PhishingEmailDetector(classifier=tuned_classifier)\n",
    "        detector.fit(X_train, y_train)\n",
    "        # detector.evaluate(X_test, y_test)\n",
    "\n",
    "        y_pred = detector.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        fitness = fitness_function(accuracy, precision, recall, f1)\n",
    "        # fitness = 0.8  # Placeholder for actual fitness calculation\n",
    "        print(f\"Classifier: {name}, Params: {param_dict}, Fitness: {fitness:.2f}\")\n",
    "\n",
    "        filename = f\"{name}_{'_'.join([f'{k}_{v}' for k, v in param_dict.items()])}.pkl\"\n",
    "        filename2 = f\"{name}_{'_'.join([f'{k}_{v}' for k, v in param_dict.items()])}_detector.pkl\"\n",
    "        # Save the model using joblib\n",
    "        joblib.dump(detector.get_pipeline(), filename)\n",
    "        joblib.dump(detector, filename2)\n",
    "\n",
    "        if fitness > best_overall_score:\n",
    "            best_overall_score = fitness\n",
    "            best_overall_model = detector\n",
    "            best_overall_name = name\n",
    "\n",
    "        # increment progress bar\n",
    "        # tqdm.write(f\"Classifier: {name}, Params: {param_dict}, Fitness: {fitness:.2f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        # tqdm.update(1)\n",
    "\n",
    "# finalize progress bar\n",
    "# tqdm.close()\n",
    "# Print the best overall model and its score\n",
    "\n",
    "print(f\"Best overall model: {best_overall_name}, Score: {best_overall_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
